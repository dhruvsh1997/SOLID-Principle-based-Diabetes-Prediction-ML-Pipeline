# -*- coding: utf-8 -*-
"""OOPs&TreeBasedMLAlgo.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ggRekhAgAAMQ9wQlxRqq3kaMim94NnX0
"""

import numpy as np
import polars as pl
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.feature_selection import VarianceThreshold
from sklearn.decomposition import PCA
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report
import xgboost as xgb
import seaborn as sns
import matplotlib.pyplot as plt
from scipy import stats
import uuid
import pickle
#Dataset from : - https://www.kaggle.com/datasets/nanditapore/healthcare-diabetes?resource=download
# Set random seed for reproducibility
np.random.seed(42)

class DataLoader:
    def __init__(self, file_path):
        self.file_path = ""
        self.df = None
        self.quotes = ["Stay healthy", "Live well", "Stay active", "Eat balanced", "Keep calm"]
        self.cities = ["New York", "Los Angeles", "Chicago", "Houston", "Phoenix"]
        self.outcome_mapping = {0: "No Diabetes", 1: "Diabetes"}

    def fetch_dataset(self, file_path):
        self.df = pl.read_csv(self.file_path)
        self.df = self.df.with_columns([
            pl.Series("Quote", np.random.choice(self.quotes, size=self.df.height)).cast(pl.Utf8),
            pl.Series("City", np.random.choice(self.cities, size=self.df.height)).cast(pl.Utf8)
        ])

    def __printShape(self):
        print("Dataset Shape:", self.df.shape)

    def _printHead(self):
        print("Dataset Head:")
        print(self.df.head())

    @staticmethod
    def editAttribute(self):
        self.df = self.df.with_columns(
            pl.col("Outcome").map_elements(lambda x: self.outcome_mapping.get(x, None)).cast(pl.Utf8).alias("Outcome")
        )

    def load_dataset(self, file_path):
        self.file_path = file_path
        self.fetch_dataset(file_path)
        self.editAttribute(self)
        self.__printShape()
        return self.df

    def __del__(self):
        print(f"DataLoader Object Deleted with file_path, quotes, cities, outcome_mapping. ")

dl=DataLoader("/content/Healthcare-Diabetes.csv")
df=dl.load_dataset("/content/Healthcare-Diabetes.csv")
dl._printHead()

#printing shape through Name mangling
dl._DataLoader__printShape()
dl.__del__()

df.shape

class DataCleaner:
    def __init__(self, df):
        self.df = df
        self.numeric_cols=[]

    @classmethod
    def check_count(cls):
        print("Null counts:\n", df.null_count())

    def replace_null(self):
      # Replace nulls with mean for numeric columns
      self.numeric_cols = [col for col, dtype in zip(self.df.columns, self.df.dtypes) if dtype in [pl.Float64, pl.Int64]]
      for col in self.numeric_cols:
          if self.df[col].null_count() > 0:
              self.df = self.df.with_columns(
                  pl.col(col).fill_null(pl.col(col).mean()).alias(col)
              )

    def __printHead(self):
        print("Dataset Head:")
        print(self.df.head())

    def __printShape(self):
        print("Dataset Shape:", self.df.height, self.df.width)

    def _check_remove_duplicates(self):
        print("Duplicate rows:", self.df.is_duplicated().sum())
        self.df = self.df.unique()

    def cleanData(self):
        self.check_count()
        self.replace_null()
        self._check_remove_duplicates()
        self.__printShape()
        return self.df

    def __del__(self):
        print("DataCleaner object deleted")

class DataOutlier(DataCleaner):
    def __init__(self, df):
        super().__init__(df)
        self.df=self.cleanData()
        self.df_pandas = self.df.to_pandas()
        self.threshold = 3

    def cal_z_score(self):
        z_scores = np.abs(stats.zscore(self.df_pandas[self.numeric_cols]))
        outlier_mask = (z_scores < self.threshold).all(axis=1)
        return outlier_mask

    def _printOutlier(self):
        print("Calculate Outlier:", self.cal_z_score())

    @staticmethod
    def statistical_info(self):
        print("Statistical summary:\n", self.df.describe())

    def remove_outliers(self):
        outlier_mask = self.cal_z_score()
        self.df_pandas = self.df_pandas[outlier_mask]
        print(self.df_pandas.head(2))
        self._printOutlier()
        return pl.from_pandas(self.df_pandas)

    def __del__(self):
        print("DataOutlier object deleted")

class DataNormaliseStandardise(DataOutlier):
    def __init__(self, df):
        super().__init__(df)
        self.df=self.remove_outliers()
        self.categorical_cols = ['Quote', 'City', 'Outcome']
        self.le = LabelEncoder()
        self.original_mappings = {}
        self.scaler = StandardScaler()

    def normaliseCatagoricalLabel(self):
        for col in self.categorical_cols:
          self.df = self.df.with_columns(
              pl.Series(col, self.le.fit_transform(self.df[col].to_pandas())).cast(pl.UInt32).alias(col)
          )
          self.original_mappings[col] = dict(zip(self.le.classes_, range(len(self.le.classes_))))
        print("\n",self.df.head(2),"\nLabel Encoded mappings:", self.original_mappings)
        #print statistial info from parent class
        super().statistical_info(self)

    def sort_rearrange(self):
      print(f"Sort By Age: {np.array(self.df['Age'].unique().sort())}")

    def __showHead(self):
      print(self.df.head(2), "\n\nRearrange by BloodPressure:")
      print(self.df.with_columns(pl.col("BloodPressure").cast(pl.UInt32).alias("BloodPressure_encoded")).head(2))
      print("\n",pl.Categorical)

    def preprocess_standardise(self):
        #Normalise
        self.normaliseCatagoricalLabel()
        self.df_pandas = self.df.to_pandas()
        # Standard Scaling
        self.df_pandas[self.numeric_cols] = self.scaler.fit_transform(self.df_pandas[self.numeric_cols])
        self.sort_rearrange()
        return pl.from_pandas(self.df_pandas)

    def __del__(self):
        # Save StandardScaler
        with open("scaler.pkl", "wb") as f:
            pickle.dump(dns.scaler, f)
        print("DataNormaliseStandardise object deleted")

dns=DataNormaliseStandardise(df)
df=dns.preprocess_standardise()
dns._DataCleaner__printShape()

dns._DataNormaliseStandardise__showHead()
# Save LabelEncoders (we reconstruct them from `original_mappings`)
label_encoders = {}
for col in dns.categorical_cols:
    le = LabelEncoder()
    le.classes_ = np.array(list(dns.original_mappings[col].keys()))
    label_encoders[col] = le

with open("label_encoders.pkl", "wb") as f:
    pickle.dump(label_encoders, f)

dns.__del__()

df.head()

class FeatureEngineering(DataNormaliseStandardise):
    numarical_cols=[]
    def __init__(self, df):
        self.df = df
        self.selected_cols=[]

    @classmethod
    def calc_col(cls):
      cls.numeric_cols = [col for col, dtype in zip(df.columns, df.dtypes) if dtype in [pl.Float64, pl.Int64]]
      print("Numeric columns:", cls.numeric_cols)
      print(df.select(pl.col("Age").unique()).to_numpy().flatten())
      return cls.numeric_cols

    @staticmethod
    def variance_threshold(self):
      selector = VarianceThreshold(threshold=0.1)
      df_numeric = self.df.select(self.numeric_cols).to_pandas()
      selected_features = selector.fit_transform(df_numeric)
      self.selected_cols = [FeatureEngineering.numeric_cols[i] for i in selector.get_support(indices=True)]
      print("Selected features after VarianceThreshold:", self.selected_cols)

    @staticmethod
    def correlation_filter(self):
      corr_matrix = self.df.select(self.selected_cols).to_pandas().corr()
      high_corr_cols = set()
      for i in range(len(corr_matrix.columns)):
          for j in range(i):
              if abs(corr_matrix.iloc[i, j]) > 0.8:
                  high_corr_cols.add(corr_matrix.columns[i])
      self.selected_cols = [col for col in self.selected_cols if col not in high_corr_cols]
      print("Selected features after correlation filter:", self.selected_cols)

    #Instance method
    def selection_n_drop(self):
      self.calc_col()
      self.variance_threshold(self)
      self.correlation_filter(self)
      # Drop string columns (Quote, City) if not already dropped
      self.df = self.df.drop(['Quote', 'City'])
      plt.figure(figsize=(7, 4))
      sns.heatmap(self.df.select(self.selected_cols).to_pandas().corr(), annot=True, cmap='coolwarm', fmt='.2f')
      plt.title('Correlation Heatmap')
      plt.show()

    @staticmethod
    def extraction_u_pca(self):
      if len(self.selected_cols) > 100:
          pca = PCA(n_components=0.95)  # Retain 95% variance
          df_scaled = pca.fit_transform(self.df_scaled.to_pandas())
          print("PCA applied, new feature count:", df_scaled.shape[1])
          df_scaled = pl.from_pandas(pd.DataFrame(df_scaled, columns=[f"PC{i+1}" for i in range(df_scaled.shape[1])]))
          return df_scaled
      else:
          print("No PCA needed, feature count:", len(self.selected_cols))
          return self.df

    def select_n_extract(self):
      self.selection_n_drop()
      self.df=self.extraction_u_pca(self)
      print("\nFeature Scaled Data:")
      print(self.df.head(2))
      return self.df

    def __del__(self):
        with open("selected_cols.pkl", "wb") as f:
          pickle.dump(self.selected_cols, f)
        print("FeatureEngineering object deleted")

dfse=FeatureEngineering(df)
df=dfse.select_n_extract()

dfse.__del__()
dfse.numarical_cols

df.head()

class TrainTest:
    def __init__(self, df):
        self.df = df

    # Define the function to accept *args
    def train_test_split(self, *args):
        # Split the DataFrame
        train_df = self.df[:args[0]]
        test_df = self.df[args[0]:]
        return train_df, test_df

    # Define the function to accept **kwargs
    def xy_split(self, **kwargs):
        split_index = int(len(self.df) * 0.7)  # 70% for training
        train_df, test_df = self.train_test_split(split_index)

        # Prepare features and target
        y_train = train_df["Outcome"]
        X_train = train_df.drop("Outcome")  # No axis argument needed
        y_test = test_df["Outcome"]
        X_test = test_df.drop("Outcome")  # No axis argument needed

        # Print additional kwargs
        for key, value in kwargs.items():
            print(f"{key}: {value}")

        return X_train, X_test, y_train, y_test


    def split_data(self):
        X_train, X_test, y_train, y_test = self.xy_split(df_length=len(self.df), df_mean=self.df["Age"].mean())

        # Print shapes
        print("Train shape:", X_train.shape, "Test shape:", X_test.shape)
        return X_train, X_test, y_train, y_test

    def __del__(self):
        print("TrainTest object deleted")

dtt=TrainTest(df)
X_train, X_test, y_train, y_test=dtt.split_data()

dtt.__del__()

from abc import ABC, abstractmethod
class ML_Model(ABC):
    @abstractmethod
    def __init__(self, X_train, X_test, y_train, y_test):
        self.X_train = X_train
        self.X_test = X_test
        self.y_train = y_train
        self.y_test = y_test

    @abstractmethod
    def train_model(self):
        pass

    @abstractmethod
    def evaluate_model(self):
        pass

    @abstractmethod
    def visualize_results(self):
        pass

    @classmethod
    @abstractmethod
    def save_model(cls, model, file_path):
        pass

    @abstractmethod
    def __del__(self):
        pass

class DecisionTree(ML_Model):
    y_pred = []

    def __init__(self, X_train, X_test, y_train, y_test):
        super().__init__(X_train, X_test, y_train, y_test)
        self.model = DecisionTreeClassifier(random_state=42)

    def train_model(self):
        self.model.fit(self.X_train, self.y_train)
        print("Decision Tree model trained.")

    def evaluate_model(self):
        DecisionTree.y_pred = self.model.predict(self.X_test)
        # Calculate metrics
        acc = accuracy_score(self.y_test, DecisionTree.y_pred)
        prec = precision_score(self.y_test, DecisionTree.y_pred, average='weighted')
        rec = recall_score(self.y_test, DecisionTree.y_pred, average='weighted')
        f1 = f1_score(self.y_test, DecisionTree.y_pred, average='weighted')

        print(f"\nDecision Tree Results:")
        print(f"Accuracy: {acc:.4f}")
        print(f"Precision: {prec:.4f}")
        print(f"Recall: {rec:.4f}")
        print(f"F1-Score: {f1:.4f}")

    def visualize_results(self):
        cm = confusion_matrix(self.y_test, DecisionTree.y_pred)
        plt.figure(figsize=(6, 4))
        sns.heatmap(cm, annot=True, cmap='Blues', fmt='d', xticklabels=['No Diabetes', 'Diabetes'], yticklabels=['No Diabetes', 'Diabetes'])
        plt.title('Decision Tree Confusion Matrix')
        plt.xlabel('Predicted')
        plt.ylabel('Actual')
        plt.show()

    @classmethod
    def save_model(cls, model, file_path):
        with open(file_path, 'wb') as file:
            pickle.dump(model, file)
        print(f"Model saved to {file_path}")

    def __del__(self):
        print("DecisionTree object deleted")

mod_dt = DecisionTree(X_train, X_test, y_train, y_test)
mod_dt.train_model()
mod_dt.evaluate_model()
mod_dt.visualize_results()
DecisionTree.save_model(mod_dt.model, 'decision_tree_model.pkl')

class RandomForest(ML_Model):
    y_pred = []
    def __init__(self, X_train, X_test, y_train, y_test):
        super().__init__(X_train, X_test, y_train, y_test)
        self.model = RandomForestClassifier(random_state=42, n_estimators=100, max_depth=10)

    def train_model(self):
        self.model.fit(self.X_train, self.y_train)
        print("Random Forest model trained.")

    def evaluate_model(self):
        RandomForest.y_pred = self.model.predict(self.X_test)
        # Calculate metrics
        acc = accuracy_score(self.y_test, RandomForest.y_pred)
        prec = precision_score(self.y_test, RandomForest.y_pred, average='weighted')
        rec = recall_score(self.y_test, RandomForest.y_pred, average='weighted')
        f1 = f1_score(self.y_test, RandomForest.y_pred, average='weighted')

        print(f"\nRandom Forest Results:")
        print(f"Accuracy: {acc:.4f}")
        print(f"Precision: {prec:.4f}")
        print(f"Recall: {rec:.4f}")
        print(f"F1-Score: {f1:.4f}")

    def visualize_results(self):
        cm = confusion_matrix(self.y_test, RandomForest.y_pred)
        plt.figure(figsize=(6, 4))
        sns.heatmap(cm, annot=True, cmap='Blues', fmt='d', xticklabels=['No Diabetes', 'Diabetes'], yticklabels=['No Diabetes', 'Diabetes'])
        plt.title('Random Forest Confusion Matrix')
        plt.xlabel('Predicted')
        plt.ylabel('Actual')
        plt.show()

    @classmethod
    def save_model(cls, model, file_path):
        with open(file_path, 'wb') as file:
            pickle.dump(model, file)
        print(f"Model saved to {file_path}")

    def __del__(self):
        print("RandomForest object deleted")

mod_rf = RandomForest(X_train, X_test, y_train, y_test)
mod_rf.train_model()
mod_rf.evaluate_model()
mod_rf.visualize_results()
RandomForest.save_model(mod_rf.model, 'random_forest_model.pkl')

class XGBoost(ML_Model):
    y_pred = []
    def __init__(self, X_train, X_test, y_train, y_test):
        super().__init__(X_train, X_test, y_train, y_test)
        self.model = xgb.XGBClassifier(random_state=42, n_estimators=100, learning_rate=0.1, max_depth=4)

    def train_model(self):
        self.model.fit(self.X_train, self.y_train)
        print("XGBoost model trained.")

    def evaluate_model(self):
        XGBoost.y_pred = self.model.predict(self.X_test)
        # Calculate metrics
        acc = accuracy_score(self.y_test, XGBoost.y_pred)
        prec = precision_score(self.y_test, XGBoost.y_pred, average='weighted')
        rec = recall_score(self.y_test, XGBoost.y_pred, average='weighted')
        f1 = f1_score(self.y_test, XGBoost.y_pred, average='weighted')
        print(f"\nXGBoost Results:")
        print(f"Accuracy: {acc:.4f}")
        print(f"Precision: {prec:.4f}")
        print(f"Recall: {rec:.4f}")
        print(f"F1-Score: {f1:.4f}")

    def visualize_results(self):
        cm = confusion_matrix(self.y_test, XGBoost.y_pred)
        plt.figure(figsize=(6, 4))
        sns.heatmap(cm, annot=True, cmap='Blues', fmt='d', xticklabels=['No Diabetes', 'Diabetes'], yticklabels=['No Diabetes', 'Diabetes'])
        plt.title('XGBoost Confusion Matrix')
        plt.xlabel('Predicted')
        plt.ylabel('Actual')
        plt.show()

    @classmethod
    def save_model(cls, model, file_path):
        with open(file_path, 'wb') as file:
            pickle.dump(model, file)
        print(f"Model saved to {file_path}")

    def __del__(self):
        print("XGBoost object deleted")

mod_xgb = XGBoost(X_train, X_test, y_train, y_test)
mod_xgb.train_model()
mod_xgb.evaluate_model()
mod_xgb.visualize_results()
XGBoost.save_model(mod_xgb.model, 'xgboost_model.pkl')



from sklearn.preprocessing import LabelEncoder, StandardScaler
model = pickle.load(open("xgboost_model.pkl", "rb"))
scaler = pickle.load(open("scaler.pkl", "rb"))
label_encoders = pickle.load(open("label_encoders.pkl", "rb"))  # Dict of encoders
selected_columns = pickle.load(open("selected_cols.pkl", "rb"))

# --- Manual input ---
sample_input = {
    'Id': 12, # Added dummy Id
    'Pregnancies': 3,
    'Glucose': 140,
    'BloodPressure': 70,
    'SkinThickness': 20,
    'Insulin': 85,
    'BMI': 32.4,
    'DiabetesPedigreeFunction': 0.6,
    'Age': 45,
    'Quote': "Live well",        # Categorical Optional (may or may not provided by default it will be "Live well")
    'City': "Houston",           # Categorical Optional (may or may not provided by default it will be "Houston")
    'Outcome': "Diabetes"        # Needed only if you want to compare
}

# --- Convert to DataFrame ---
sample_df = pd.DataFrame([sample_input])
sample_df = pl.from_pandas(sample_df)

# --- Encode categorical using saved label_encoders ---
for col in ['Quote', 'City', 'Outcome']:
    le = label_encoders[col]
    sample_df = sample_df.with_columns(
        pl.Series(col, le.transform(sample_df[col].to_pandas())).cast(pl.UInt32).alias(col)
    )

# --- Scale numeric columns ---
numeric_cols = ['Id', 'Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age'] # Added Id to numeric_cols
sample_df_pandas = sample_df.to_pandas()
sample_df_pandas[numeric_cols] = scaler.transform(sample_df_pandas[numeric_cols])

# --- Convert back to Polars ---
sample_df = pl.from_pandas(sample_df_pandas)

# --- Select only final features ---
sample_df = sample_df.select(selected_columns)

# --- Predict ---
prediction = model.predict(sample_df.to_pandas())[0]
pred_label = "Diabetes" if prediction == 1 else "No Diabetes"

print(f"Prediction: {pred_label}")

